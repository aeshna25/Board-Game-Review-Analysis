{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas\n",
    "import matplotlib\n",
    "import seaborn \n",
    "import sklearn \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILDING THE  DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games = pandas.read_csv('board_games.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the name of columns in the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(games.columns)\n",
    "print(games.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a histagram of all the ratings in the 'average_weight' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(games['average_weight'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have an avg of 0 a lot which is not good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRINT THE FIRST ROW OF ALL THE GAMES HAVING 0 SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here printing name by indexing(0)\n",
    "print(games[games['average_rating']==0].iloc[0])\n",
    "#(to compare)print the games having score more than 0\n",
    "print(games[games['average_rating']>0].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all rows with no user review and/or missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games =games[games['users_rated']>0] #no review\n",
    "games =games.dropna(axis=0)          #missing data\n",
    "#histagram\n",
    "plt.hist(games['average_rating'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CORRELATION MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat =games.corr()\n",
    "fig =plt.figure(figsize =(12,9))\n",
    "\n",
    "sns.heatmap(corrmat,vmax= .8,square =True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above heatmap its shown the correaltion b/w different columns of the dataset. Though some of the columns are not needed(id,type,name). So to make it more precise these columns needs to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all the data from the dataframe\n",
    "columns =games.columns.tolist()\n",
    "#filter the data from the data we dont want\n",
    "columns =[c for c in columns if c not in ['bayes_average_rating','average_rating','type','name','id']]\n",
    "#store the variable we will be predicting on\n",
    "target ='average_rating'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train =games.sample(frac =.8 , random_state =1)\n",
    "\n",
    "#select anything which is not part of the training set to go in test set\n",
    "test =games.loc[~games.index.isin(train.index)]\n",
    "#print the shape\n",
    "print(train.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model  import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#initalize the model class\n",
    "LR =LinearRegression()\n",
    "\n",
    "#optimize the regressor by fitting the model in our training data\n",
    "LR.fit(train[columns],train[target])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate predictions for the test sets\n",
    "predictions =LR.predict(test[columns])\n",
    "#compute  error between test predictions and actual data\n",
    "mean_squared_error(predictions,test[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since even from the eat map it was clear that there was not much linear relationship between the models so trying non-linear relationship (decision tree) approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random forest model(decision  tree)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#initalize the model\n",
    "RFR = RandomForestRegressor(n_estimators=100, min_samples_leaf =10,random_state =1)\n",
    "#fit to the data\n",
    "RFR.fit(train[columns],train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate predictions for our test sets\n",
    "predictions = RFR.predict(test[columns])\n",
    "#compute the error between test predictions and actual data\n",
    "mean_squared_error(predictions,test[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NON-LINEAR REGRESSION MODEL GIVES LESS ERROR THAN THE LR \n",
    "\n",
    "NOW CHECKING FOR INDIVIDUAL CASES RATHER THAN WHOLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking game one features\n",
    "test[columns].iloc[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting the result by using both LR and RFR model\n",
    "rating_LR = LR.predict(test[columns].iloc[0].values.reshape(1,-1))\n",
    "rating_RFR =RFR.predict(test[columns].iloc[0].values.reshape(1,-1))\n",
    "\n",
    "#print\n",
    "print(rating_LR)\n",
    "print(rating_RFR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print actual value\n",
    "test[target].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the LR model is more closer to the actual value. And its impressive that even after 15709 people reviewed it we  were able to reach a no. closer to the actual value by predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
